{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d04f18-7b93-4459-ba10-3b1bbbce2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0079f7c-f296-4dac-aede-cae0c32b0941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    141067\n",
      "0    121187\n",
      "3     57317\n",
      "4     47712\n",
      "2     34554\n",
      "5     14972\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel like theres way too much im trying to a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have been feeling gloomy since monday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i feel that anger toward someone else not cari...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i try not to think about it because identifyin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i feel so at peace and less stressed now that ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  i feel like theres way too much im trying to a...      3\n",
       "1            i have been feeling gloomy since monday      0\n",
       "2  i feel that anger toward someone else not cari...      2\n",
       "3  i try not to think about it because identifyin...      0\n",
       "4  i feel so at peace and less stressed now that ...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train-00000-of-00001.csv\")\n",
    "\n",
    "print(df['label'].value_counts())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b195bfcd-a9d2-4008-ae6c-9c8ffb677c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Aman\n",
      "[nltk_data]     Jha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aman\n",
      "[nltk_data]     Jha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55d1a911-dc59-4aaa-b393-977a0fde95dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "negations = {\"no\", \"not\", \"nor\", \"never\",\"didnt\",\"don't\",\"didnt\", \"doesn't\", \"didn't\",\n",
    "             \"hadn't\", \"won't\", \"can't\", \"couldn't\", \"shouldn't\" ,\n",
    "             \"wouldn't\", \"ain't\", \"left\"}\n",
    "stop = stop - negations\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word for word in text.split() if word not in stop])\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a0d9d1-1907-4409-8a1c-89cff4ff9b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (333447,)\n",
      "X_test shape: (83362,)\n",
      "y_train shape: (333447,)\n",
      "y_test shape: (83362,)\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(clean_text)\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "num_classes = len(le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9628e77b-77a1-41f2-9da5-a582da10decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 25000\n",
    "MAX_LEN = 70\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11817e8c-289b-49be-8e1c-4b22f39056d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = EmotionDataset(X_train_pad, y_train_enc)\n",
    "test_ds = EmotionDataset(X_test_pad, y_test_enc)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "145b2ec6-cd5c-467b-8ab2-8ff73cb7d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "class_counts = df['label'].value_counts().sort_index()\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = [class_weights[y] for y in y_train]\n",
    "\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "train_loader = DataLoader(train_ds, batch_size=64, sampler=sampler)\n",
    "embedding_index = {}\n",
    "vocab_size = min(MAX_NUM_WORDS, len(tokenizer.word_index) + 1)\n",
    "\n",
    "with open(\"glove.6B.100d.txt\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < vocab_size:\n",
    "        vector = embedding_index.get(word)\n",
    "        if vector is not None:\n",
    "            embedding_matrix[i] = vector\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b898e54-5e39-460d-bbd7-bdf5bef2c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class SentimentBiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float),\n",
    "            freeze=False, \n",
    "            padding_idx=0\n",
    "        )\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attention = nn.Linear(hidden_dim*2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        out, _ = self.lstm(emb)\n",
    "        attn_weights = torch.softmax(self.attention(out), dim=1)\n",
    "        context = torch.sum(attn_weights * out, dim=1)\n",
    "        context = self.dropout(context)\n",
    "        return self.fc(context)\n",
    "\n",
    "model = SentimentBiLSTM(vocab_size, embed_dim=100, hidden_dim=128, num_classes=num_classes).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "094cb497-36b2-4c0b-900f-9eef9bf689ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = min(MAX_NUM_WORDS, len(tokenizer.word_index)+1)\n",
    "model = SentimentBiLSTM(vocab_size, embed_dim=100, hidden_dim=128, num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1104c4d4-eeb5-4a22-bd70-ac8432637ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d8ab1e3-6299-4995-9c62-7b3717f21c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_acc=0.9322, val_acc=0.9388, val_loss=0.1221\n",
      "Epoch 2: train_acc=0.9554, val_acc=0.9390, val_loss=0.1158\n",
      "Epoch 3: train_acc=0.9568, val_acc=0.9395, val_loss=0.1125\n",
      "Epoch 4: train_acc=0.9570, val_acc=0.9384, val_loss=0.1233\n",
      "Epoch 5: train_acc=0.9573, val_acc=0.9366, val_loss=0.1227\n",
      "Epoch 6: train_acc=0.9583, val_acc=0.9379, val_loss=0.1275\n",
      "Epoch 7: train_acc=0.9591, val_acc=0.9353, val_loss=0.1335\n",
      "Epoch 8: train_acc=0.9596, val_acc=0.9343, val_loss=0.1288\n",
      "⏹ Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "early_stopper = EarlyStopping(patience=5)  \n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_correct += (logits.argmax(1) == yb).sum().item()\n",
    "    train_acc = total_correct / len(train_ds)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, val_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            val_loss += loss.item()\n",
    "            val_correct += (logits.argmax(1) == yb).sum().item()\n",
    "    val_acc = val_correct / len(test_ds)\n",
    "            \n",
    "    print(f\"Epoch {epoch+1}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}, val_loss={val_loss/len(test_loader):.4f}\")\n",
    "\n",
    "    early_stopper(val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"⏹ Early stopping triggered\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b700caca-fbae-47e5-b431-3ec89e39913b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9342746095343202\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.97     24414\n",
      "           1       1.00      0.91      0.95     28058\n",
      "           2       0.77      1.00      0.87      6863\n",
      "           3       0.92      0.96      0.94     11454\n",
      "           4       0.88      0.88      0.88      9562\n",
      "           5       0.71      0.90      0.79      3011\n",
      "\n",
      "    accuracy                           0.93     83362\n",
      "   macro avg       0.88      0.93      0.90     83362\n",
      "weighted avg       0.94      0.93      0.94     83362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    target_names=[str(c) for c in le.classes_]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f96c0-41f5-4a8a-8a35-752637dbf169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a088a49-0c31-432d-9cfe-4052aee37510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
